{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4009daf8",
   "metadata": {},
   "source": [
    "# Serve Ollama Model via API with Localtunnel\n",
    "\n",
    "This notebook guides you through serving a chosen Ollama model and exposing its API publicly using localtunnel.\n",
    "\n",
    "**Steps:**\n",
    "1. Install necessary packages.\n",
    "2. Check and manage your Ollama installation and models.\n",
    "3. Select or pull the Ollama model you want to serve.\n",
    "4. Create a FastAPI server to provide an API for the model.\n",
    "5. Test the local API.\n",
    "6. Expose the local API to the internet using localtunnel.\n",
    "7. Learn how to use and manage the exposed API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a48e13",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "We need `ollama` (the Python client), `fastapi` for the web server, `uvicorn` to run FastAPI, `pydantic` for data validation, `nest_asyncio` to allow `uvicorn` to run in a Jupyter environment, and `localtunnel` (via npm) to expose the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages\n",
    "print(\"Installing Python packages: ollama, fastapi, uvicorn, pydantic, nest_asyncio\")\n",
    "!pip install ollama fastapi uvicorn pydantic nest_asyncio\n",
    "\n",
    "# Install localtunnel globally using npm\n",
    "print(\"Installing localtunnel globally using npm...\")\n",
    "!npm install -g localtunnel\n",
    "\n",
    "print(\"Dependencies installed.\")\n",
    "\n",
    "# nest_asyncio is used to allow uvicorn to run in a Jupyter notebook environment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27729681",
   "metadata": {},
   "source": [
    "## 2. Check Ollama Installation and Running Status\n",
    "\n",
    "This step ensures Ollama is installed and the Ollama server is running on your system. If not, it will attempt to start the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c789c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def check_ollama():\n",
    "    print(\"Checking Ollama installation...\")\n",
    "    try:\n",
    "        version_output = subprocess.check_output([\"ollama\", \"--version\"], text=True, stderr=subprocess.STDOUT)\n",
    "        print(f\"Ollama is installed: {version_output.strip()}\")\n",
    "    except (subprocess.SubprocessError, FileNotFoundError) as e:\n",
    "        print(f\"Ollama not found or error during version check: {e}\")\n",
    "        print(\"Please install Ollama from https://ollama.com/download and ensure it's in your PATH.\")\n",
    "        sys.exit(\"Ollama installation check failed.\")\n",
    "\n",
    "    print(\"\\nChecking Ollama server status...\")\n",
    "    try:\n",
    "        subprocess.check_output([\"ollama\", \"list\"], text=True, timeout=10) # Increased timeout for initial check\n",
    "        print(\"Ollama server is running and responsive.\")\n",
    "    except (subprocess.SubprocessError, FileNotFoundError, subprocess.TimeoutExpired) as e:\n",
    "        print(f\"Ollama server not responding or not found: {e}\")\n",
    "        print(\"Attempting to start Ollama server...\")\n",
    "        try:\n",
    "            if sys.platform == \"win32\":\n",
    "                subprocess.Popen([\"ollama\", \"serve\"], creationflags=subprocess.CREATE_NEW_CONSOLE)\n",
    "            else:\n",
    "                # Start Ollama server in the background, detaching it\n",
    "                subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, start_new_session=True)\n",
    "            \n",
    "            print(\"Waiting for Ollama server to start (15 seconds)...\")\n",
    "            time.sleep(15) # Give server more time to start\n",
    "            \n",
    "            # Verify again\n",
    "            subprocess.check_output([\"ollama\", \"list\"], text=True, timeout=10)\n",
    "            print(\"Ollama server started and is responsive.\")\n",
    "        except (subprocess.SubprocessError, FileNotFoundError, subprocess.TimeoutExpired) as start_err:\n",
    "            print(f\"Failed to start or verify Ollama server: {start_err}\")\n",
    "            print(\"Please ensure Ollama is installed correctly and can be run from the command line.\")\n",
    "            print(\"You might need to start 'ollama serve' manually in a separate terminal if issues persist.\")\n",
    "            sys.exit(\"Ollama server startup failed.\")\n",
    "\n",
    "check_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07164b3d",
   "metadata": {},
   "source": [
    "## 3. List Available Models and Select/Pull Model\n",
    "\n",
    "Let's see which models you have locally. If the model you want isn't listed, you can pull it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee32fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def list_local_models():\n",
    "    print(\"\\nListing available local models...\")\n",
    "    try:\n",
    "        models_info = ollama.list()\n",
    "        if not models_info['models']:\n",
    "            print(\"  No models found locally.\")\n",
    "            return []\n",
    "        print(\"Available models:\")\n",
    "        for i, model_data in enumerate(models_info['models']):\n",
    "            size_gb = model_data['size'] / (1024**3)\n",
    "            print(f\"  {i+1}. {model_data['name']} (Size: {size_gb:.2f} GB)\")\n",
    "        return [m['name'] for m in models_info['models']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        print(\"Make sure the Ollama server is running (see previous step).\")\n",
    "        return []\n",
    "\n",
    "local_model_names = list_local_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "print(\"\\n--- Model Selection ---\")\n",
    "if local_model_names:\n",
    "    print(\"You can choose from your local models or pull a new one.\")\n",
    "else:\n",
    "    print(\"No local models found. You'll need to pull a model.\")\n",
    "\n",
    "model_to_serve = input(\"Enter the name of the Ollama model you want to serve (e.g., 'llama3:8b', 'mistral', 'codellama'): \").strip()\n",
    "\n",
    "if not model_to_serve:\n",
    "    print(\"No model name entered. Exiting.\")\n",
    "    sys.exit(\"Model name required.\")\n",
    "\n",
    "if model_to_serve not in local_model_names:\n",
    "    print(f\"Model '{model_to_serve}' not found locally.\")\n",
    "    pull_choice = input(f\"Do you want to pull '{model_to_serve}'? This can take some time and disk space. (yes/no): \").strip().lower()\n",
    "    if pull_choice == 'yes':\n",
    "        print(f\"Pulling '{model_to_serve}'... Please be patient.\")\n",
    "        try:\n",
    "            # Stream the pull progress\n",
    "            current_digest = \"\"\n",
    "            for progress in ollama.pull(model_to_serve, stream=True):\n",
    "                digest = progress.get(\"digest\", \"\")\n",
    "                if digest != current_digest and digest != \"\":\n",
    "                    current_digest = digest\n",
    "                    print(f\"Pulling {digest} - {progress.get('status')}\")\n",
    "                if \"total\" in progress and \"completed\" in progress:\n",
    "                    percentage = (progress['completed'] / progress['total']) * 100\n",
    "                    print(f\"Status: {progress.get('status')} - {percentage:.2f}% completed\", end='\\r')\n",
    "            print(\"\\nPull completed.\")\n",
    "            print(f\"Successfully pulled '{model_to_serve}'.\")\n",
    "            local_model_names.append(model_to_serve) # Add to list for consistency\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError pulling model '{model_to_serve}': {e}\")\n",
    "            print(\"Please check the model name (e.g., 'llama2', 'mistral:latest') and your internet connection.\")\n",
    "            sys.exit(\"Model pull failed.\")\n",
    "    else:\n",
    "        print(\"Model not pulled. Exiting.\")\n",
    "        sys.exit(\"No model selected to serve.\")\n",
    "else:\n",
    "    print(f\"Using local model: '{model_to_serve}'\")\n",
    "\n",
    "SELECTED_MODEL = model_to_serve\n",
    "print(f\"Will attempt to serve the model: {SELECTED_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6533beb",
   "metadata": {},
   "source": [
    "## 4. Create FastAPI Server\n",
    "\n",
    "We'll create a simple API server using FastAPI to interact with the selected Ollama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Body\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "import threading\n",
    "import ollama # Ensure ollama is imported here as well for the API functions\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "import asyncio # For uvicorn shutdown\n",
    "\n",
    "# Define request/response models for FastAPI\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "    model: Optional[str] = None # If None, uses SELECTED_MODEL from notebook scope\n",
    "    images: Optional[List[str]] = Field(default=None, description=\"A list of base64-encoded images for multimodal models\")\n",
    "    format: Optional[str] = Field(default=None, description=\"The format to return a response in. Currently the only accepted value is 'json'\")\n",
    "    options: Optional[Dict[str, Any]] = Field(default=None, description=\"Additional model parameters listed in the documentation for the Modelfile such as temperature\")\n",
    "    system: Optional[str] = Field(default=None, description=\"System message to (overrides what is defined in the Modelfile)\")\n",
    "    template: Optional[str] = Field(default=None, description=\"The full prompt or prompt template (overrides what is defined in the Modelfile)\")\n",
    "    context: Optional[List[int]] = Field(default=None, description=\"The context parameter returned from a previous request to /generate, used to keep a short conversational memory\")\n",
    "    stream: Optional[bool] = Field(default=False, description=\"If false the response will be returned as a single response object, rather than a stream of objects\")\n",
    "    keep_alive: Optional[Union[str, float, int]] = Field(default=None, description=\"Controls how long the model will stay loaded into memory following the request (default: 5m)\")\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    model: str\n",
    "    created_at: str\n",
    "    response: str\n",
    "    done: bool\n",
    "    context: Optional[List[int]] = None\n",
    "    total_duration: Optional[int] = None\n",
    "    load_duration: Optional[int] = None\n",
    "    prompt_eval_count: Optional[int] = None\n",
    "    prompt_eval_duration: Optional[int] = None\n",
    "    eval_count: Optional[int] = None\n",
    "    eval_duration: Optional[int] = None\n",
    "\n",
    "app = FastAPI(title=\"Ollama Model Server\", description=f\"Serving Ollama model: {SELECTED_MODEL}\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": f\"Ollama API server for model '{SELECTED_MODEL}' is running. Use the /api/generate endpoint.\"}\n",
    "\n",
    "@app.post(\"/api/generate\", response_model=GenerateResponse)\n",
    "async def generate_text(request: GenerateRequest):\n",
    "    try:\n",
    "        model_name_to_use = request.model if request.model else SELECTED_MODEL\n",
    "        \n",
    "        payload = request.dict(exclude_none=True) # Create payload from request, excluding unset fields\n",
    "        payload['model'] = model_name_to_use # Ensure model is in payload\n",
    "        \n",
    "        if payload.get('stream', False):\n",
    "            # This basic endpoint is not designed for true streaming to the HTTP client.\n",
    "            # Ollama client itself handles the stream and aggregates if stream=True is passed to it\n",
    "            # but the FastAPI response model expects a single object.\n",
    "            # For simplicity, we will let ollama.generate handle it; it returns the full response if stream=True but not handled by client.\n",
    "            print(\"Warning: 'stream: True' requested. The ollama client will aggregate the stream into a single response for this endpoint.\")\n",
    "\n",
    "        print(f\"Received request for model '{model_name_to_use}' with prompt: '{request.prompt[:50]}...'\")\n",
    "        response_data = ollama.generate(**payload)\n",
    "        print(f\"Generated response. Done: {response_data.get('done')}\")\n",
    "        return response_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/active-model\")\n",
    "def get_active_model():\n",
    "    return {\"active_model\": SELECTED_MODEL, \"status\": \"ready\"}\n",
    "\n",
    "# --- Server Start/Stop --- (Global state for server management)\n",
    "PORT = 8008\n",
    "uvicorn_server_instance = None\n",
    "server_thread = None\n",
    "\n",
    "def start_fastapi_server():\n",
    "    global uvicorn_server_instance, server_thread\n",
    "    if server_thread is not None and server_thread.is_alive():\n",
    "        print(f\"FastAPI server is already running or starting on port {PORT}.\")\n",
    "        return\n",
    "\n",
    "    config = uvicorn.Config(app, host=\"127.0.0.1\", port=PORT, log_level=\"info\")\n",
    "    uvicorn_server_instance = uvicorn.Server(config)\n",
    "    \n",
    "    server_thread = threading.Thread(target=uvicorn_server_instance.run, daemon=True)\n",
    "    server_thread.start()\n",
    "    print(f\"FastAPI server starting on http://127.0.0.1:{PORT}\")\n",
    "    time.sleep(3) # Wait a bit for the server to initialize\n",
    "    if server_thread.is_alive():\n",
    "        print(f\"FastAPI server started successfully for model '{SELECTED_MODEL}'. Access it locally.\")\n",
    "    else:\n",
    "        print(\"FastAPI server failed to start. Check console for errors.\")\n",
    "        uvicorn_server_instance = None # Clear instance if failed\n",
    "\n",
    "# Automatically start the server when this cell is run\n",
    "start_fastapi_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24acdd",
   "metadata": {},
   "source": [
    "## 5. Test Local API Server\n",
    "\n",
    "Before exposing it, let's test if the local API server is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_local_api():\n",
    "    print(\"\\n--- Testing Local API Server ---\")\n",
    "    if server_thread and server_thread.is_alive():\n",
    "        base_url = f\"http://127.0.0.1:{PORT}\"\n",
    "        # Test root endpoint\n",
    "        try:\n",
    "            response_root = requests.get(base_url, timeout=10)\n",
    "            response_root.raise_for_status()\n",
    "            print(f\"GET /: {response_root.json()}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error testing GET /: {e}\")\n",
    "            return\n",
    "\n",
    "        # Test /api/active-model endpoint\n",
    "        try:\n",
    "            response_active = requests.get(f\"{base_url}/api/active-model\", timeout=10)\n",
    "            response_active.raise_for_status()\n",
    "            print(f\"GET /api/active-model: {response_active.json()}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error testing GET /api/active-model: {e}\")\n",
    "            return\n",
    "\n",
    "        # Test /api/generate endpoint\n",
    "        api_generate_url = f\"{base_url}/api/generate\"\n",
    "        test_payload = {\n",
    "            \"prompt\": \"Why is the sky blue? Explain briefly.\",\n",
    "            # \"model\": SELECTED_MODEL, # Not needed as API defaults to SELECTED_MODEL\n",
    "            \"options\": {\"temperature\": 0.7}\n",
    "        }\n",
    "        try:\n",
    "            print(f\"POST /api/generate with prompt: '{test_payload['prompt']}'\")\n",
    "            response_generate = requests.post(api_generate_url, json=test_payload, timeout=120) # Longer timeout for generation\n",
    "            response_generate.raise_for_status() \n",
    "            \n",
    "            response_data = response_generate.json()\n",
    "            print(\"Local API /generate Test Response:\")\n",
    "            print(f\"  Model: {response_data.get('model')}\")\n",
    "            print(f\"  Response: {response_data.get('response')[:300]}...\")\n",
    "            if not response_data.get('response'):\n",
    "                 print(\"  Warning: Empty response from model. The model might be very slow or there could be an issue.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error testing POST /api/generate: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON from API response. Status: {response_generate.status_code}, Response text: {response_generate.text}\")\n",
    "    else:\n",
    "        print(\"FastAPI server is not running. Please run the server cell (Cell 10) first.\")\n",
    "\n",
    "# Run the test\n",
    "test_local_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ad442",
   "metadata": {},
   "source": [
    "## 6. Expose API with Localtunnel\n",
    "\n",
    "Now, let's make your local API server accessible from the internet using localtunnel. This will output a public URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab85c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import re\n",
    "\n",
    "public_url_store = {\"url\": None} # Use a dictionary to store mutable URL\n",
    "lt_process_store = {\"process\": None} # Store the process object\n",
    "\n",
    "def monitor_localtunnel_output(proc, url_store):\n",
    "    try:\n",
    "        for line in iter(proc.stdout.readline, ''):\n",
    "            line_stripped = line.strip()\n",
    "            print(f\"Localtunnel: {line_stripped}\")\n",
    "            url_match = re.search(r'your url is: (https?://[^\\s]+)', line_stripped)\n",
    "            if url_match:\n",
    "                url_store[\"url\"] = url_match.group(1)\n",
    "                print(f\"\\n🎉 Public API URL: {url_store['url']}\")\n",
    "                print(f\"You can now access your Ollama model '{SELECTED_MODEL}' API at this URL.\")\n",
    "                print(f\"Example: POST to {url_store['url']}/api/generate\")\n",
    "                # Don't close stdout, let it run to show connection status or errors\n",
    "        proc.stdout.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading localtunnel stdout: {e}\")\n",
    "\n",
    "def start_localtunnel(port_to_expose, url_store, process_store):\n",
    "    if process_store[\"process\"] and process_store[\"process\"].poll() is None:\n",
    "        print(f\"Localtunnel is already running or starting.\")\n",
    "        if url_store[\"url\"]:\n",
    "            print(f\"Current Public URL: {url_store['url']}\")\n",
    "        else:\n",
    "            print(\"Localtunnel is running, but URL not yet captured. Check output.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Starting Localtunnel for port {port_to_expose} ---\")\n",
    "    try:\n",
    "        command = [\"lt\", \"--port\", str(port_to_expose)]\n",
    "        # For subdomains, you can try: command.extend([\"--subdomain\", \"my-ollama-server\"])\n",
    "        # However, custom subdomains might be rate-limited or require payment on public localtunnel instances.\n",
    "        \n",
    "        process_store[\"process\"] = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)\n",
    "        \n",
    "        # Thread to monitor stdout for the URL without blocking\n",
    "        stdout_thread = threading.Thread(target=monitor_localtunnel_output, args=(process_store[\"process\"], url_store), daemon=True)\n",
    "        stdout_thread.start()\n",
    "\n",
    "        # Thread to monitor stderr for errors\n",
    "        def monitor_stderr(proc):\n",
    "            for line in iter(proc.stderr.readline, ''):\n",
    "                print(f\"Localtunnel ERR: {line.strip()}\", file=sys.stderr)\n",
    "            proc.stderr.close()\n",
    "        stderr_thread = threading.Thread(target=monitor_stderr, args=(process_store[\"process\"],), daemon=True)\n",
    "        stderr_thread.start()\n",
    "\n",
    "        print(\"Localtunnel process started. Waiting for URL...\")\n",
    "        # Give it a few seconds to establish and print the URL\n",
    "        time.sleep(10) \n",
    "        if not url_store[\"url\"]:\n",
    "            print(\"Localtunnel URL not detected after 10s. Check its output above.\")\n",
    "            print(\"It might be slow, or there could be an issue with localtunnel service (e.g., rate limits, server down).\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'lt' command not found. Make sure localtunnel is installed globally (npm install -g localtunnel) and in your PATH.\")\n",
    "        url_store[\"url\"] = None\n",
    "        process_store[\"process\"] = None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while starting localtunnel: {e}\")\n",
    "        url_store[\"url\"] = None\n",
    "        process_store[\"process\"] = None\n",
    "\n",
    "# Ensure the FastAPI server is running before starting localtunnel\n",
    "if server_thread and server_thread.is_alive():\n",
    "    start_localtunnel(PORT, public_url_store, lt_process_store)\n",
    "else:\n",
    "    print(\"FastAPI server is not running. Please run the server cell (Cell 10) first to expose via localtunnel.\")\n",
    "\n",
    "# Display the URL again if already set\n",
    "if public_url_store[\"url\"]:\n",
    "    print(f\"\\nYour API should be accessible at: {public_url_store['url']}\")\n",
    "    print(f\"Test with: curl -X POST {public_url_store['url']}/api/generate -H \\\"Content-Type: application/json\\\" -d '{{\\\"prompt\\\":\\\"Hi there!\\\", \\\"model\\\":\\\"{SELECTED_MODEL}\\\"}}'\")\n",
    "elif lt_process_store[\"process\"]:\n",
    "    print(\"\\nLocaltunnel is attempting to start/running. Check its output above for the URL or any errors.\")\n",
    "else:\n",
    "    print(\"\\nLocaltunnel did not start. Check for errors in the cell output.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d68b7",
   "metadata": {},
   "source": [
    "## 7. Using Your Exposed API\n",
    "\n",
    "If localtunnel started successfully, your API is live at the URL it provided.\n",
    "\n",
    "**Endpoint:** `POST {public_url}/api/generate`\n",
    "\n",
    "**Request Body (JSON):**\n",
    "Review the `GenerateRequest` model in Cell 10 for all possible parameters. A simple request:\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"Your prompt here\",\n",
    "  \"model\": \"optional_model_name_override\" \n",
    "}\n",
    "```\n",
    "\n",
    "**Example using cURL:**\n",
    "(Replace `YOUR_PUBLIC_URL` with the actual URL from localtunnel output)\n",
    "```bash\n",
    "curl -X POST YOUR_PUBLIC_URL/api/generate \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"prompt\": \"Tell me a fun fact about Large Language Models.\", \"model\": \"''' + SELECTED_MODEL + '''\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eadc94",
   "metadata": {},
   "source": [
    "## 8. Stopping the Server and Tunnel\n",
    "\n",
    "When you're done, you should stop the localtunnel and the FastAPI server.\n",
    "- **Localtunnel**: Can be stopped by running the `stop_services()` function below or by interrupting/restarting the kernel.\n",
    "- **FastAPI Server**: Running in a daemon thread, it will stop when the Jupyter kernel is shut down or restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb576e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "def stop_services():\n",
    "    global uvicorn_server_instance, server_thread, lt_process_store, public_url_store\n",
    "    \n",
    "    print(\"\\n--- Stopping Services ---\")\n",
    "    \n",
    "    # Stop localtunnel\n",
    "    lt_proc = lt_process_store.get(\"process\")\n",
    "    if lt_proc and lt_proc.poll() is None: # Check if process exists and is running\n",
    "        print(\"Terminating localtunnel process...\")\n",
    "        lt_proc.terminate() # Try to terminate gracefully\n",
    "        try:\n",
    "            lt_proc.wait(timeout=5) # Wait for it to terminate\n",
    "            print(\"Localtunnel process terminated.\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"Localtunnel process did not terminate gracefully, killing.\")\n",
    "            lt_proc.kill()\n",
    "            print(\"Localtunnel process killed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during localtunnel termination: {e}\")\n",
    "        lt_process_store[\"process\"] = None\n",
    "        public_url_store[\"url\"] = None\n",
    "    else:\n",
    "        print(\"Localtunnel process not found or already stopped.\")\n",
    "\n",
    "    # Stop FastAPI/Uvicorn server\n",
    "    # Uvicorn running in a thread needs a more direct shutdown if possible.\n",
    "    # The `should_exit` event is the recommended way for uvicorn.Server\n",
    "    if uvicorn_server_instance:\n",
    "        print(\"Requesting FastAPI server (Uvicorn) to shut down...\")\n",
    "        uvicorn_server_instance.should_exit = True\n",
    "        # Wait for the thread to join\n",
    "        if server_thread and server_thread.is_alive():\n",
    "            server_thread.join(timeout=5)\n",
    "            if server_thread.is_alive():\n",
    "                print(\"FastAPI server thread did not exit gracefully.\")\n",
    "            else:\n",
    "                print(\"FastAPI server thread exited.\")\n",
    "        uvicorn_server_instance = None\n",
    "        server_thread = None\n",
    "    else:\n",
    "        print(\"FastAPI server instance not found (might have failed to start or already stopped).\")\n",
    "    \n",
    "    print(\"Services stop requested. If issues persist, restart the Jupyter kernel.\")\n",
    "\n",
    "# You can run this cell to stop the services manually.\n",
    "# stop_services()\n",
    "\n",
    "print(\"Run the line 'stop_services()' in this cell (uncomment it) or a new cell to stop localtunnel and attempt to stop the FastAPI server.\")\n",
    "print(\"The most reliable way to stop all processes started by this notebook is to 'Restart Kernel' or 'Shut Down Kernel' from the Jupyter menu.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
